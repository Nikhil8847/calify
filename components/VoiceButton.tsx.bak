import {
  mockProcessAudio,
  TranscriptionResponse,
} from "@/services/audioService";
import { Ionicons } from "@expo/vector-icons";
import * as FileSystem from "expo-file-system";
import * as Haptics from "expo-haptics";
import React, { useEffect, useRef, useState } from "react";

// Add typings for global object to store the recorder instance
declare global {
  var _activeRecorder: AudioRecorderPlayer | null;
}
import {
  ActivityIndicator,
  Animated,
  PermissionsAndroid,
  Platform,
  StyleSheet,
  TouchableOpacity,
  View,
} from "react-native";
import AudioRecorderPlayer, {
  AVEncodingOption,
} from "react-native-audio-recorder-player";

interface VoiceButtonProps {
  onStartRecording?: () => void;
  onStopRecording?: (transcription?: TranscriptionResponse) => void;
  isProcessing?: boolean;
  size?: number;
  apiUrl?: string;
}

const VoiceButton: React.FC<VoiceButtonProps> = ({
  onStartRecording,
  onStopRecording,
  isProcessing = false,
  size = 100,
  apiUrl = "http://127.0.0.1:5000/upload",
}) => {
  const [isRecording, setIsRecording] = useState(false);
  const [animation] = useState(new Animated.Value(1));
  const [transcription, setTranscription] =
    useState<TranscriptionResponse | null>(null);
  const [silenceTimer, setSilenceTimer] = useState<ReturnType<
    typeof setTimeout
  > | null>(null);
  const [autoStopEnabled, setAutoStopEnabled] = useState(true);

  // Use useRef to maintain the AudioRecorderPlayer instance across renders
  const audioRecorderPlayerRef = useRef<AudioRecorderPlayer | null>(null);

  useEffect(() => {
    // Initialize the AudioRecorderPlayer instance
    try {
      audioRecorderPlayerRef.current = new AudioRecorderPlayer();
      console.log("AudioRecorderPlayer initialized successfully");
    } catch (error) {
      console.error("Failed to initialize AudioRecorderPlayer:", error);
    }

    // Request permissions immediately
    const setupAudio = async () => {
      try {
        await requestPermissions();
        console.log("Audio permissions requested successfully");
      } catch (error) {
        console.error("Failed to request audio permissions:", error);
      }
    };

    setupAudio();

    return () => {
      // Cleanup on unmount
      if (silenceTimer) {
        clearTimeout(silenceTimer);
      }

      // Clean up the audio recorder
      if (audioRecorderPlayerRef.current) {
        try {
          audioRecorderPlayerRef.current.removeRecordBackListener();
          console.log("AudioRecorderPlayer cleaned up");
        } catch (e) {
          console.error("Error cleaning up AudioRecorderPlayer:", e);
        }
      }
    };
  }, []);

  const requestPermissions = async () => {
    if (Platform.OS === "android") {
      const grants = await PermissionsAndroid.requestMultiple([
        PermissionsAndroid.PERMISSIONS.RECORD_AUDIO,
        PermissionsAndroid.PERMISSIONS.WRITE_EXTERNAL_STORAGE,
      ]);
      
      console.log("Android permissions result:", grants);
      
      // Check if permissions were granted
      const audioGranted = grants[PermissionsAndroid.PERMISSIONS.RECORD_AUDIO] === PermissionsAndroid.RESULTS.GRANTED;
      const storageGranted = grants[PermissionsAndroid.PERMISSIONS.WRITE_EXTERNAL_STORAGE] === PermissionsAndroid.RESULTS.GRANTED;
      
      if (!audioGranted || !storageGranted) {
        console.warn("Audio or storage permission denied");
      }
    } else if (Platform.OS === "ios") {
      // For iOS, permissions will be requested by AudioRecorderPlayer automatically
      // We can add a note in the console for debugging
      console.log("iOS will request audio permissions when recording starts");
    }
  };

  // Use FileSystem from expo instead of RNFS
  // Ensure path ends with a slash before the filename to avoid path issues
  const path = FileSystem.documentDirectory
    ? `${
        FileSystem.documentDirectory.endsWith("/")
          ? FileSystem.documentDirectory
          : FileSystem.documentDirectory + "/"
      }audio.wav`
    : "audio.wav";
  const audioSet = {
    AVSampleRateKeyIOS: 16000, // Required for Google STT
    AVFormatIDKeyIOS: AVEncodingOption.wav, // Linear PCM (WAV format)
    AVNumberOfChannelsKeyIOS: 1, // Mono audio
    AVLinearPCMBitDepthKeyIOS: 16, // 16-bit depth
    AVLinearPCMIsBigEndianKeyIOS: false,
    AVLinearPCMIsFloatKeyIOS: false,
    AudioSourceAndroid: 6, // VOICE_RECOGNITION for clear speech
    OutputFormatAndroid: 2, // Encoding format: WAV (default PCM)
    AudioEncoderAndroid: 3, // ENCODER_PCM_16BIT
    AudioEncodingBitRateAndroid: 256000, // High-quality audio
    AudioSamplingRateAndroid: 16000, // Required for Google STT
    AudioChannelsAndroid: 1, // Mono
  };

  // Function to handle silence detection
  const startSilenceDetection = () => {
    // Clear any existing timer
    if (silenceTimer) {
      clearTimeout(silenceTimer);
    }

    // Set a new timer for 2 seconds
    const timer = setTimeout(() => {
      if (isRecording && autoStopEnabled) {
        console.log("Silence detected - stopping recording");
        handleStopRecording();
      }
    }, 2000); // Stop after 2 seconds of silence

    setSilenceTimer(timer);
  };

  const handlePressIn = async () => {
    try {
      if (isRecording) {
        // If already recording, stop it (user pressed again while recording)
        console.log("Already recording, stopping...");
        handleStopRecording();
        return;
      }
  
      // Make sure AudioRecorderPlayer is initialized
      if (!audioRecorderPlayerRef.current) {
        console.log("AudioRecorderPlayer was not initialized, creating a new instance");
        
        try {
          audioRecorderPlayerRef.current = new AudioRecorderPlayer();
          // Wait a moment for initialization
          await new Promise((resolve) => setTimeout(resolve, 300));
          console.log("New AudioRecorderPlayer instance created");
        } catch (error) {
          console.error("Failed to create AudioRecorderPlayer:", error);
          return;
        }
      }
  
      // Double-check that initialization worked
      if (!audioRecorderPlayerRef.current) {
        console.error("AudioRecorderPlayer initialization failed");
        return;
      }
      
      // Explicitly request permissions again to be safe
      await requestPermissions();
    } catch (error) {
      console.error("Error in handlePressIn setup:", error);
      return;
    }

    setIsRecording(true);
    Haptics.impactAsync(Haptics.ImpactFeedbackStyle.Medium);

    // Clear any existing silence timers
    if (silenceTimer) {
      clearTimeout(silenceTimer);
      setSilenceTimer(null);
    }

    // Start pulsing animation
    Animated.loop(
      Animated.sequence([
        Animated.timing(animation, {
          toValue: 1.2,
          duration: 500,
          useNativeDriver: true,
        }),
        Animated.timing(animation, {
          toValue: 1,
          duration: 500,
          useNativeDriver: true,
        }),
      ])
    ).start();

    // Start recording
    try {
      console.log("Starting recorder with path:", path);

      // Ensure directory exists
      const dirPath = FileSystem.documentDirectory;
      console.log("Document directory:", dirPath);
      
      // Ensure we have a valid recorder instance
      if (!audioRecorderPlayerRef.current) {
        console.log("Recorder was null before starting, creating a new one");
        audioRecorderPlayerRef.current = new AudioRecorderPlayer();
        // Wait a moment for initialization to complete
        await new Promise(resolve => setTimeout(resolve, 500));
      }
      
      // Check again after possible recreation
      if (!audioRecorderPlayerRef.current) {
        throw new Error("Failed to create AudioRecorderPlayer instance");
      }
      
      // Double-check that the directory exists and is writable
      try {
        // Check if the directory exists by trying to get info
        const dirInfo = await FileSystem.getInfoAsync(FileSystem.documentDirectory);
        console.log("Directory info:", dirInfo);
      } catch (dirErr) {
        console.error("Error checking directory:", dirErr);
        // Continue anyway, the error might be false
      }
      
      console.log("About to call startRecorder with audioSet:", JSON.stringify(audioSet).substring(0, 100) + "...");
      
      // IMPORTANT: Create a local copy of the recorder instance to prevent null issues
      // if the ref changes during the async operation
      const recorderInstance = audioRecorderPlayerRef.current;
      
      // Store recorder instance in a variable to prevent it from being garbage collected
      // This is critical as some devices may be releasing the instance too early
      globalThis._activeRecorder = recorderInstance;
      
      setIsRecording(true);
      let result: string;
      
      // Direct approach without Promise wrapper
      try {
        result = await recorderInstance.startRecorder(path, audioSet);
      } catch (innerError) {
        console.error("Direct startRecorder error:", innerError);
        throw innerError;
      }

      console.log("Recording started:", result);
      onStartRecording?.();
      
      // Confirm our audioRecorderPlayer is still valid after recording started
      if (!audioRecorderPlayerRef.current) {
        throw new Error("audioRecorderPlayer became null after recording started");
      }

      // Subscribe to audio meter updates to detect silence
      audioRecorderPlayerRef.current.addRecordBackListener((e: any) => {
        // Reset the silence timer every time we hear sound above threshold
        if (e && e.currentMetering && e.currentMetering > -35) {
          startSilenceDetection();
        }
      });

      // Start initial silence detection (this will be reset when voice is detected)
      startSilenceDetection();
    } catch (error) {
      console.error("Error starting recording:", error);
      setIsRecording(false);

      // Try to recover the AudioRecorderPlayer instance
      try {
        console.log("Attempting to recover AudioRecorderPlayer instance...");
        if (audioRecorderPlayerRef.current) {
          try {
            // Try to clean up the existing instance first
            audioRecorderPlayerRef.current.removeRecordBackListener();
          } catch (e) {
            console.warn("Error cleaning up existing AudioRecorderPlayer:", e);
          }
        }
        
        // Create a fresh instance
        audioRecorderPlayerRef.current = new AudioRecorderPlayer();
        console.log("AudioRecorderPlayer instance recovered");
      } catch (e) {
        console.error("Failed to recover AudioRecorderPlayer:", e);
      }
    }
  };

  const handleStopRecording = async () => {
    if (!isRecording) return;

    if (!audioRecorderPlayerRef.current) {
      console.error("AudioRecorderPlayer is not initialized");
      setIsRecording(false); // Make sure we reset the state
      return;
    }

    setIsRecording(false);
    Haptics.impactAsync(Haptics.ImpactFeedbackStyle.Light);

    // Clear silence timer
    if (silenceTimer) {
      clearTimeout(silenceTimer);
      setSilenceTimer(null);
    }

    // Remove record back listener
    audioRecorderPlayerRef.current.removeRecordBackListener();

    // Stop animation
    animation.stopAnimation();
    Animated.timing(animation, {
      toValue: 1,
      duration: 200,
      useNativeDriver: true,
    }).start();

    try {
      // Stop recording
      console.log("Stopping recorder...");
      const result = await audioRecorderPlayerRef.current
        .stopRecorder()
        .catch((err) => {
          console.error("Inner stopRecorder error:", err);
          throw err; // Rethrow to be caught by outer catch
        });

      console.log("Recording stopped:", result);

      // Read the recorded file as binary using expo-file-system
      let fileData;
      try {
        if (result) {
          console.log("Reading file data...");
          fileData = await FileSystem.readAsStringAsync(result, {
            encoding: FileSystem.EncodingType.Base64,
          });
          console.log(
            "File data read successfully, length:",
            fileData?.length || 0
          );
        } else {
          console.warn("No file result from stopRecorder");
        }
      } catch (err) {
        console.log("Error reading file:", err);
        // Continue without file data, since we're using mock response anyway
      }

      // Convert to format for proper processing
      // Create a properly typed audio blob for React Native FormData
      const audioBlob = {
        uri: Platform.OS === "android" ? result : `file://${result}`,
        name: "audio.wav",
        type: "audio/wav",
      } as any; // Use type assertion to avoid TypeScript errors

      const formData = new FormData();
      formData.append("file", audioBlob);

      try {
        // Just use mock API for now to avoid network issues
        // We're simulating processing the audio without actually sending it
        const response = await mockProcessAudio();

        setTranscription(response);
        console.log("Mock transcription:", response);
        onStopRecording?.(response);
      } catch (error) {
        console.error("Error processing audio:", error);
        onStopRecording?.();
      }
    } catch (error) {
      console.error("Error stopping recording:", error);
      onStopRecording?.();
    }
  };

  // This function handles when user lifts their finger from button
  const handlePressOut = () => {
    // We don't automatically stop recording on press out anymore
    // Instead, we let it continue until silence is detected or button is pressed again
    console.log("Press out - continuing to record until silence detected");
  };

  const buttonSize = { width: size, height: size, borderRadius: size / 2 };

  return (
    <View style={styles.container}>
      <Animated.View
        style={[
          styles.buttonShadow,
          buttonSize,
          { transform: [{ scale: animation }] },
        ]}
      >
        <TouchableOpacity
          onPress={handlePressIn}
          activeOpacity={0.8}
          disabled={isProcessing}
          style={[
            styles.button,
            buttonSize,
            isRecording && styles.recording,
            isProcessing && styles.processing,
          ]}
        >
          {isProcessing ? (
            <ActivityIndicator color="#FFFFFF" size="small" />
          ) : (
            <Ionicons
              name={isRecording ? "mic" : "mic-outline"}
              size={size / 2}
              color="#FFFFFF"
            />
          )}
        </TouchableOpacity>
      </Animated.View>
    </View>
  );
};

const styles = StyleSheet.create({
  container: {
    alignItems: "center",
    justifyContent: "center",
    position: "absolute",
    bottom: 15,
    alignSelf: "center",
    zIndex: 999,
  },
  buttonShadow: {
    backgroundColor: "transparent",
    shadowColor: "#000",
    shadowOffset: { width: 0, height: 2 },
    shadowOpacity: 0.25,
    shadowRadius: 5,
    elevation: 5,
  },
  button: {
    backgroundColor: "#30D07B",
    alignItems: "center",
    justifyContent: "center",
  },
  recording: {
    backgroundColor: "#FF6347", // Tomato red when recording
  },
  processing: {
    backgroundColor: "#FFA500", // Orange when processing
  },
  hintText: {
    fontSize: 10,
    color: "#666",
    marginTop: 5,
    textAlign: "center",
  },
});

export default VoiceButton;
